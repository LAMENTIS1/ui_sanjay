<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Voice Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 100vh;
            background-color: #f0f2f5;
            color: #333;
            padding: 20px;
        }
        
        .container {
            width: 100%;
            max-width: 500px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }
        
        .mic-button {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background-color: #4285f4;
            border: none;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }
        
        .mic-button svg {
            width: 40px;
            height: 40px;
            fill: white;
            transition: all 0.3s ease;
        }
        
        .mic-button.listening {
            background-color: #ea4335;
            animation: pulse 1.5s infinite;
        }
        
        .mic-button.speaking {
            background-color: #34a853;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
        
        .transcript-container {
            width: 100%;
            min-height: 100px;
            max-height: 200px;
            overflow-y: auto;
            background-color: white;
            border-radius: 12px;
            padding: 16px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            transition: opacity 0.3s ease;
            opacity: 0;
            margin-top: 20px;
        }
        
        .transcript-container.active {
            opacity: 1;
        }
        
        .status {
            font-size: 14px;
            color: #666;
            text-align: center;
            height: 20px;
            margin-top: 10px;
        }
        
        /* Touch ripple effect */
        .ripple {
            position: absolute;
            border-radius: 50%;
            background-color: rgba(255, 255, 255, 0.4);
            transform: scale(0);
            animation: ripple 0.6s linear;
        }
        
        @keyframes ripple {
            to {
                transform: scale(2.5);
                opacity: 0;
            }
        }
        
        /* Responsive styles */
        @media (max-width: 768px) {
            .mic-button {
                width: 70px;
                height: 70px;
            }
            
            .mic-button svg {
                width: 35px;
                height: 35px;
            }
            
            .transcript-container {
                font-size: 15px;
            }
        }
        
        @media (max-width: 480px) {
            .mic-button {
                width: 60px;
                height: 60px;
            }
            
            .mic-button svg {
                width: 30px;
                height: 30px;
            }
            
            .transcript-container {
                font-size: 14px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <button id="micButton" class="mic-button">
            <svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
            </svg>
        </button>
        
        <div id="status" class="status">Tap to speak</div>
        
        <div id="transcriptContainer" class="transcript-container">
            <div id="transcriptText"></div>
        </div>
    </div>

    <script>
        // DOM Elements
        const micButton = document.getElementById('micButton');
        const status = document.getElementById('status');
        const transcriptContainer = document.getElementById('transcriptContainer');
        const transcriptText = document.getElementById('transcriptText');
        
        // State variables
        let recognition = null;
        let isListening = false; // Tracks if speech recognition is active
        let isListeningFlag = false; // Flag to control listening state
        let isProcessingAPI = false;
        let speaking = false;
        let silenceTimeout = null;
        let speechSynthesis = window.speechSynthesis;
        
        // Initialize Speech Recognition
        function initializeSpeechRecognition() {
            if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
                status.textContent = 'Speech recognition not supported in this browser';
                return null;
            }
            
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            const newRecognition = new SpeechRecognition();
            newRecognition.continuous = true; // Keep listening until explicitly stopped
            newRecognition.interimResults = true; // Get interim results
            newRecognition.maxAlternatives = 1;
            newRecognition.lang = 'en-US';
            
            return newRecognition;
        }
        
        // Start listening
        function startListening() {
            if (!isListeningFlag || isListening || isProcessingAPI) return;
            
            try {
                if (recognition) {
                    recognition.stop();
                }
                
                recognition = initializeSpeechRecognition();
                if (!recognition) {
                    status.textContent = 'Could not initialize speech recognition';
                    return;
                }
                
                // Update UI
                micButton.classList.add('listening');
                micButton.classList.remove('speaking');
                status.textContent = 'Listening...';
                transcriptContainer.classList.add('active');
                transcriptText.textContent = '';
                
                // Set up event handlers
                recognition.onstart = function() {
                    isListening = true;
                    console.log('Speech recognition started');
                };
                
                recognition.onresult = function(event) {
                    clearTimeout(silenceTimeout);
                    const lastResult = event.results[event.results.length - 1];
                    const transcript = lastResult[0].transcript.trim();
                    const isFinalResult = lastResult.isFinal;
                    
                    transcriptText.textContent = transcript;
                    
                    // If it's a final result or silence is detected, process it
                    if (isFinalResult) {
                        processFinalTranscript(transcript);
                    } else {
                        // Set a timeout for silence detection (5 seconds for mobile devices)
                        silenceTimeout = setTimeout(() => {
                            processFinalTranscript(transcript);
                        }, 5000); // Increased silence timeout to 5 seconds
                    }
                };
                
                recognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    if (event.error === 'no-speech') {
                        // No speech detected, continue listening
                        restartRecognition();
                    } else if (event.error === 'aborted' || event.error === 'network') {
                        stopListening();
                    }
                };
                
                recognition.onend = function() {
                    // If ended but we still want to be listening, restart
                    if (isListeningFlag && !isProcessingAPI) {
                        restartRecognition();
                    } else {
                        if (!isProcessingAPI && !speaking) {
                            transcriptContainer.classList.remove('active');
                            setTimeout(() => {
                                transcriptText.textContent = '';
                            }, 300);
                        }
                    }
                };
                
                recognition.start();
            } catch (error) {
                console.error('Failed to start speech recognition:', error);
                status.textContent = 'Error starting recognition';
                isListening = false;
                micButton.classList.remove('listening');
            }
        }
        
        // Stop listening
        function stopListening() {
            if (recognition) {
                isListening = false;
                recognition.stop();
                micButton.classList.remove('listening');
                status.textContent = 'Tap to speak';
                
                if (!isProcessingAPI && !speaking) {
                    transcriptContainer.classList.remove('active');
                }
            }
        }
        
        // Process final transcript
        function processFinalTranscript(transcript) {
            if (!transcript) return;
            
            console.log('Processing final transcript:', transcript);
            
            // Set the processing flag to true
            isProcessingAPI = true;
            
            // Stop listening since we're now processing
            if (isListening) {
                stopListening();
            }
            
            // Update UI
            status.textContent = 'Processing...';
            
            // Cancel any ongoing speech
            if (speaking) {
                speechSynthesis.cancel();
                speaking = false;
                micButton.classList.remove('speaking');
            }
            
            // Send the transcript to the API
            query({ question: transcript })
                .then(response => {
                    console.log('API response:', response);
                    
                    // Display the response
                    transcriptText.textContent = response.text;
                    status.textContent = 'Response';
                    
                    // Speak the response using text-to-speech
                    speakResponse(response.text);
                })
                .catch(error => {
                    console.error('API error:', error);
                    transcriptText.textContent = 'Sorry, there was an error processing your request.';
                    status.textContent = 'Error';
                    speakResponse('Sorry, there was an error processing your request.');
                })
                .finally(() => {
                    // Reset the processing flag and turn off the listening flag
                    isProcessingAPI = false;
                    isListeningFlag = false;
                });
        }
        
        // Text-to-speech function
        function speakResponse(text) {
            if (!('speechSynthesis' in window)) {
                console.error('Speech synthesis not supported in this browser');
                return;
            }
            
            // Create a new speech synthesis utterance
            const utterance = new SpeechSynthesisUtterance(text);
            
            // Set voice, rate, pitch
            utterance.rate = 1.0;
            utterance.pitch = 1.0;
            utterance.volume = 1.0;
            
            // Try to select a good voice
            const voices = speechSynthesis.getVoices();
            const preferredVoice = voices.find(voice => 
                voice.name.includes('Female') && 
                (voice.lang.includes('en-US') || voice.lang.includes('en-GB'))
            ) || voices[0];
            
            if (preferredVoice) {
                utterance.voice = preferredVoice;
            }
            
            // Event listeners
            utterance.onstart = () => {
                speaking = true;
                status.textContent = 'Speaking...';
                micButton.classList.add('speaking');
            };
            
            utterance.onend = () => {
                speaking = false;
                micButton.classList.remove('speaking');
                status.textContent = 'Tap to speak';
                
                // Hide transcript after speaking
                setTimeout(() => {
                    if (!isListening) {
                        transcriptContainer.classList.remove('active');
                        setTimeout(() => {
                            transcriptText.textContent = '';
                        }, 300);
                    }
                }, 1000);
            };
            
            utterance.onerror = (event) => {
                speaking = false;
                micButton.classList.remove('speaking');
                status.textContent = 'Error speaking';
                console.error('Speech synthesis error:', event);
            };
            
            // Start speaking
            speechSynthesis.speak(utterance);
        }
        
        // API query function - using your existing endpoint
        async function query(data) {
            try {
                const response = await fetch(
                    "https://srivatsavdamaraju-flowise.hf.space/api/v1/prediction/2875301a-c26f-4bd5-ab10-71fa13393541", 
                    {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json"
                        },
                        body: JSON.stringify(data)
                    }
                );
                
                if (!response.ok) {
                    throw new Error(`API responded with status: ${response.status}`);
                }
                
                return await response.json();
            } catch (error) {
                console.error('API query error:', error);
                throw error;
            }
        }
        
        // Initialize the app
        function init() {
            // Make sure we have the latest voices loaded
            if ('speechSynthesis' in window) {
                window.speechSynthesis.onvoiceschanged = () => {
                    console.log('Voices loaded:', window.speechSynthesis.getVoices().length);
                };
            }
            
            // Request microphone permission early
            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                navigator.mediaDevices.getUserMedia({ audio: true })
                    .then(() => {
                        console.log('Microphone permission granted');
                    })
                    .catch(error => {
                        console.error('Microphone permission denied:', error);
                        status.textContent = 'Microphone access denied';
                    });
            }
            
            // Set up click/touch handler for the mic button
            micButton.addEventListener('click', (event) => {
                if (speaking) {
                    // If currently speaking, stop speech
                    stopSpeaking();
                } else if (isListening) {
                    // If currently listening, stop listening
                    stopListening();
                } else if (!isProcessingAPI) {
                    // If not processing, toggle the listening flag and start listening
                    isListeningFlag = !isListeningFlag;
                    if (isListeningFlag) {
                        startListening();
                    } else {
                        stopListening();
                    }
                }
            });
        }
        
        // Initialize when page loads
        document.addEventListener('DOMContentLoaded', init);
    </script>
</body>
</html>
